{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **LIBRARY**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library sistem\n",
    "import os\n",
    "import gdown\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Google spreadsheet API\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "# Library dasar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Library visualisasi\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "# Library pre-processing data\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Library statistika\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "# Library pemodelan\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Bidirectional, GRU, Dropout, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "# Library evaluasi\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Library penyetelan hiperparameter\n",
    "# import optuna\n",
    "# from optuna.visualization import (\n",
    "#     plot_optimization_history,\n",
    "#     plot_param_importances,\n",
    "#     plot_parallel_coordinate,\n",
    "#     plot_slice,\n",
    "#     plot_contour\n",
    "# )\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **CONFIGURATION**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output directory ready: ../hasil/pemodelan\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Configuration\n",
    "# ============================\n",
    "METADATA_PATH = \"../data/processed/(K3 - Transformasi per jam) Beban listrik.csv\" # Pastikan File telah ditambahkan\n",
    "OUTPUT_PATH = \"../hasil/pemodelan\"\n",
    "\n",
    "# Membuat direktori jika belum ada\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "print(f\"Output directory ready: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **LOAD METADATA**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memanggil data\n",
    "df_siap = pd.read_csv(METADATA_PATH)\n",
    "df_siap['TANGGAL_JAM'] = pd.to_datetime(df_siap['TANGGAL_JAM'])\n",
    "df_siap['TAHUN'] = df_siap['TANGGAL_JAM'].dt.year\n",
    "df_use = df_siap[(df_siap['TAHUN'] >= 2016) & (df_siap['TAHUN'] <= 2025)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# **DEEP LEARNING**\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Spliting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buat_dataset(data, time_step=1):\n",
    "  X, y = [], []\n",
    "  for i in range(len(data) - time_step):\n",
    "    a = data[i:(i + time_step), 0]\n",
    "    X.append(a)\n",
    "    y.append(data[i + time_step, 0])\n",
    "  return np.array(X), np.array(y)\n",
    "\n",
    "def bagi_data(df, ukuran_pelatihan, time_step, scaler):\n",
    "  data = df['BEBAN'].values.reshape(-1, 1)\n",
    "  scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "  X, y = buat_dataset(scaled_data, time_step)\n",
    "\n",
    "  X_train, X_test = X[:ukuran_pelatihan], X[ukuran_pelatihan:]\n",
    "  y_train, y_test = y[:ukuran_pelatihan], y[ukuran_pelatihan:]\n",
    "\n",
    "  train_data = pd.concat([pd.DataFrame(X_train), pd.DataFrame(y_train)], axis=1)\n",
    "  test_data = pd.concat([pd.DataFrame(X_test), pd.DataFrame(y_test)], axis=1)\n",
    "\n",
    "  X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "  X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "  return train_data, test_data, X_train, X_test, y_train, y_test, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "UKURAN_PELATIHAN = int(len(df_use) * 0.8)\n",
    "\n",
    "time_step = 168 #(1 hari)\n",
    "\n",
    "train_data, test_data, X_train, X_test, y_train, y_test, scaler = bagi_data(df_use, UKURAN_PELATIHAN, time_step, MinMaxScaler(feature_range=(0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total : 77857\n",
      "Ukuran pelatihan : 62285\n",
      "X : 77689\n",
      "Train : 62285 (62285,62285)\n",
      "Test : 15404 (15404,15404)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total : {len(df_use)}\")\n",
    "print(f\"Ukuran pelatihan : {UKURAN_PELATIHAN}\")\n",
    "print(f\"X : {len(X_train) + len(X_test)}\")\n",
    "print(f\"Train : {len(train_data)} ({len(X_train)},{len(y_train)})\")\n",
    "print(f\"Test : {len(test_data)} ({len(X_test)},{len(y_test)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pemodelan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fungsi untuk membangun arsitektur model LSTM\n",
    "def build_lstm(layer, dense, dropout_rate, time_step):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(time_step,1)))\n",
    "  for i in range(0, layer):\n",
    "      model.add(LSTM(dense[i], return_sequences=(i < layer - 1)))\n",
    "      model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "  return model, early_stopping\n",
    "\n",
    "# Fungsi untuk membangun arsitektur model BiLSTM\n",
    "def build_bilstm(layer, dense, dropout_rate, time_step):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(time_step,1)))\n",
    "  for i in range(0, layer):\n",
    "      model.add(Bidirectional(LSTM(dense[i],  return_sequences=(i < layer - 1))))\n",
    "      model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "  return model, early_stopping\n",
    "\n",
    "# Fungsi untuk membangun arsitektur model GRU\n",
    "def build_gru(layer, dense, dropout_rate, time_step):\n",
    "  model = Sequential()\n",
    "  model.add(Input(shape=(time_step,1)))\n",
    "  for i in range(0, layer):\n",
    "      model.add(GRU(dense[i], return_sequences=(i < layer - 1)))\n",
    "      model.add(Dropout(dropout_rate))\n",
    "  model.add(Dense(1))\n",
    "  model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True)\n",
    "  return model, early_stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model yang dibangun di bawah ini hanyalah model-model uji coba saja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **LSTM**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 168, 116)          54752     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 168, 116)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 104)               91936     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 104)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 105       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 146,793\n",
      "Trainable params: 146,793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer = 2\n",
    "dense = [116, 104]\n",
    "dropout_rate = 0.1\n",
    "epochs = 3\n",
    "batch_size = 512\n",
    "\n",
    "model_lstm, early_stooping = build_lstm(layer, dense, dropout_rate, time_step)\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "122/122 [==============================] - 11s 70ms/step - loss: 0.0150 - val_loss: 0.0107\n",
      "Epoch 2/3\n",
      "122/122 [==============================] - 8s 65ms/step - loss: 0.0059 - val_loss: 0.0035\n",
      "Epoch 3/3\n",
      "122/122 [==============================] - 8s 64ms/step - loss: 0.0034 - val_loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stooping], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **BiLSTM**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "layer = 3\n",
    "dense = [70,70,70]\n",
    "dropout_rate = 0.110570919753144\n",
    "epochs = 3\n",
    "batch_size = int(len(train_data) / 25)\n",
    "\n",
    "model_bilstm, early_stooping = build_bilstm(layer, dense, dropout_rate, time_step)\n",
    "model_bilstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Graph execution error:\n\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 140, 70, 1, 168, 2491, 70] \n\t [[{{node CudnnRNN}}]]\n\t [[sequential_3/bidirectional_1/forward_lstm_7/PartitionedCall]] [Op:__inference_train_function_28939]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_bilstm \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_bilstm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stooping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Lab Matematika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Lab Matematika\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInternalError\u001b[0m: Graph execution error:\n\nFailed to call ThenRnnForward with model config: [rnn_mode, rnn_input_mode, rnn_direction_mode]: 2, 0, 0 , [num_layers, input_size, num_units, dir_count, max_seq_length, batch_size, cell_num_units]: [1, 140, 70, 1, 168, 2491, 70] \n\t [[{{node CudnnRNN}}]]\n\t [[sequential_3/bidirectional_1/forward_lstm_7/PartitionedCall]] [Op:__inference_train_function_28939]"
     ]
    }
   ],
   "source": [
    "history_bilstm = model_bilstm.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stooping], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **GRU**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 2\n",
    "dense = [116, 104]\n",
    "dropout_rate = 0.1\n",
    "epochs = 161\n",
    "batch_size = 512\n",
    "\n",
    "model_gru, early_stooping = build_gru(layer, dense, dropout_rate, time_step)\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_gru = model_gru.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[early_stooping], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **EVALUASI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mae(y_true, y_pred, scaler):\n",
    "    mae_scaled = mean_absolute_error(y_true, y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return mae_scaled, mae\n",
    "\n",
    "def mse(y_true, y_pred, scaler):\n",
    "    mse_scaled = mean_squared_error(y_true, y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    return mse_scaled, mse\n",
    "\n",
    "def mape(y_true, y_pred, scaler):\n",
    "    mape_scaled = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    return mape_scaled, mape\n",
    "\n",
    "def rmse(y_true, y_pred, scaler):\n",
    "    rmse_scaled = root_mean_squared_error(y_true, y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    return rmse_scaled, rmse\n",
    "\n",
    "def r2(y_true, y_pred, scaler):\n",
    "    r2_scaled = r2_score(y_true, y_pred)\n",
    "    y_true = scaler.inverse_transform(y_true)\n",
    "    y_pred = scaler.inverse_transform(y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return r2\n",
    "\n",
    "def simpan_loss(history_data, title):\n",
    "    loss_history = history_data.history['loss']\n",
    "    val_loss_history = history_data.history['val_loss']\n",
    "    epochs = range(1, len(loss_history) + 1)\n",
    "\n",
    "    best_epoch = min(range(len(val_loss_history)), key=val_loss_history.__getitem__) + 1\n",
    "    best_val_loss = min(val_loss_history)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'Epoch': epochs,\n",
    "        'Training Loss': loss_history,\n",
    "        'Validation Loss': val_loss_history\n",
    "    })\n",
    "\n",
    "    best_epoch_data = pd.DataFrame({\n",
    "        'Epoch': [best_epoch],\n",
    "        'Training Loss': [None],\n",
    "        'Validation Loss': [best_val_loss]\n",
    "    })\n",
    "\n",
    "    # Concatenate best epoch row\n",
    "    df = pd.concat([df, best_epoch_data], ignore_index=True)\n",
    "\n",
    "    df.to_csv(f'Output/History/{title}.csv', index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def evaluasi(X_train, X_test, y_train, y_test, time_step, model, scaler, save=[False], verbose=False):\n",
    "    train_predict = model.predict(X_train, verbose=0)\n",
    "    test_predict = model.predict(X_test, verbose=0)\n",
    "\n",
    "    train_data_new = y_train.reshape(-1, 1)\n",
    "    test_data_new = y_test.reshape(-1, 1)\n",
    "\n",
    "    mae_train_scaled, mae_train = mae(train_data_new,train_predict,scaler)\n",
    "    mae_test_scaled, mae_test = mae(test_data_new,test_predict,scaler)\n",
    "    mse_train_scaled, mse_train = mse(train_data_new,train_predict,scaler)\n",
    "    mse_test_scaled, mse_test = mse(test_data_new,test_predict,scaler)\n",
    "    rmse_train_scaled, rmse_train = rmse(train_data_new,train_predict,scaler)\n",
    "    rmse_test_scaled, rmse_test = rmse(test_data_new,test_predict,scaler)\n",
    "    mape_train_scaled, mape_train = mape(train_data_new,train_predict,scaler)\n",
    "    mape_test_scaled, mape_test = mape(test_data_new,test_predict,scaler)\n",
    "    r2_train = r2(train_data_new,train_predict,scaler)\n",
    "    r2_test = r2(test_data_new,test_predict,scaler)\n",
    "\n",
    "    if(save[0]):\n",
    "        print(\"Saving history\")\n",
    "        _, model, time_step, layer, dense, dropout_rate, epochs, batch_size = save\n",
    "        history = [model, time_step, layer, str(dense), dropout_rate, epochs, batch_size, mse_train, mse_test, mae_train, mae_test, rmse_train, rmse_test, mape_train, mape_test, r2_train, r2_test]\n",
    "\n",
    "        try:\n",
    "            simpan_histori(client, history)\n",
    "        except Exception as e:\n",
    "            print(f\"Error appending row: {e}\")\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"Matriks Evaluasi : Train\\tTest\")\n",
    "        print(f\"MSE : {mse_train}\\t{mse_test}\")\n",
    "        print(f\"MAE : {mae_train}\\t{mae_test}\")\n",
    "        print(f\"RMSE : {rmse_train}\\t{rmse_test}\")\n",
    "        print(f\"MAPE : {mape_train}\\t{mape_test}\")\n",
    "        print(f\"R2 : {r2_train}\\t{r2_test}\")\n",
    "        return mae_train, mae_test\n",
    "\n",
    "    return mse_train, mse_test\n",
    "\n",
    "def plot_evaluasi(title, time, data, train_predict, test_predict, time_step, scaler):\n",
    "    train_predict_rescaled = scaler.inverse_transform(train_predict)\n",
    "    test_predict_rescaled = scaler.inverse_transform(test_predict)\n",
    "\n",
    "    empty_values = np.full((time_step, train_predict_rescaled.shape[1]), np.nan)  # Membuat array kosong (NaN) dengan panjang 24\n",
    "    train_predict_rescaled = np.concatenate((empty_values, train_predict_rescaled), axis=0)\n",
    "\n",
    "    # Menggabungkan prediksi pelatihan dengan prediksi pengujian\n",
    "    all_data = np.concatenate((train_predict_rescaled, test_predict_rescaled), axis=0).reshape(-1)\n",
    "\n",
    "    # Membuat DataFrame dengan kolom waktu dan nilai yang sesuai\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'time': time,\n",
    "        'data_asli': data,\n",
    "        'prediksi': all_data\n",
    "    })\n",
    "\n",
    "    predictions_df.to_csv(f\"Output/Data/Prediksi/prediksi_{title[0]}_{time_step}.csv\", index=False)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time, data, color='b', label='Data asli', lw=0.5)\n",
    "    plt.plot(time[time_step:len(train_predict)+time_step], scaler.inverse_transform(train_predict), color='orange', label='Prediksi data pelatihan', alpha=0.95, lw=0.5)\n",
    "    plt.plot(time[len(train_predict)+time_step:len(data)], scaler.inverse_transform(test_predict), color='r', label='Prediksi data pengujian', alpha=0.95,lw=0.5)\n",
    "    plt.xlabel('Tanggal')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Beban Listrik (MW)')\n",
    "    plt.title(f\"Prediksi {title[0]}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediksi(title, time, data, train_predict, test_predict, time_step, scaler, days):\n",
    "    n_data = len(train_predict)+len(test_predict)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(time[n_data-days+time_step:n_data+days+time_step], data[n_data-days+time_step:n_data+days+time_step], color='b', label='Data asli')\n",
    "    plt.plot(time[n_data-days+time_step:n_data+days+time_step], scaler.inverse_transform(test_predict)[len(test_predict)-days:len(test_predict)], color='r', label='Prediksi data pengujian', alpha=0.8)\n",
    "    plt.xlabel('Tanggal')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.locator_params(axis='x', nbins=days)\n",
    "    plt.ylabel('Beban Listrik (MW)')\n",
    "    plt.title(f\"Model {title[0]}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_prediksi_df(title, time, data, predict, time_step, day1, day2, alpha, lw):\n",
    "    plt.figure(figsize=(16,5))\n",
    "    ukuran_pelatihan = int(len(data)*0.8)\n",
    "\n",
    "    if ukuran_pelatihan < day1:\n",
    "        ukuran_pelatihan = day1\n",
    "\n",
    "    if ukuran_pelatihan > day2:\n",
    "        ukuran_pelatihan = day2\n",
    "\n",
    "    plt.plot(time[day1:day2], data[day1:day2], color='blue', label='Data asli')\n",
    "\n",
    "    if ukuran_pelatihan > day1:\n",
    "        plt.plot(time[day1:ukuran_pelatihan], predict[day1:ukuran_pelatihan], color='orange', label='Prediksi data latih', alpha=alpha, lw=lw)\n",
    "    if ukuran_pelatihan < day2:\n",
    "        plt.plot(time[ukuran_pelatihan:day2], predict[ukuran_pelatihan:day2], color='red', label='Prediksi data uji', alpha=alpha, lw=lw)\n",
    "\n",
    "    plt.xlabel('Tanggal')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel('Beban Listrik (MW)')\n",
    "    plt.title(f\"Prediksi {title[0]}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(title, history, epochs):\n",
    "    plt.plot(history.history['loss'][:epochs], label='Error pelatihan')\n",
    "    plt.plot(history.history['val_loss'][:epochs], label='Eror pengujian')\n",
    "    plt.title(title[0])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_df(title, history, epochs):\n",
    "    plt.plot(history['Training Loss'][:epochs], label='Error pelatihan')\n",
    "    plt.plot(history['Validation Loss'][:epochs], label='Eror pengujian')\n",
    "    plt.title(f\"Kurva pelatihan {title[0]}\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error (MSE data normalisasi)')\n",
    "    plt.legend()\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/{title[1]}.pdf\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss_gabungan(df_lstm, df_bilstm, df_gru, time_step, ep1,ep2):\n",
    "    # Mengambil data epoch dan loss pelatihan dan pengujian dari dataframe\n",
    "    epochs_lstm = df_lstm['Epoch']\n",
    "    loss_lstm_train = df_lstm['Training Loss']\n",
    "    loss_lstm_val = df_lstm['Validation Loss']\n",
    "\n",
    "    epochs_bilstm = df_bilstm['Epoch']\n",
    "    loss_bilstm_train = df_bilstm['Training Loss']\n",
    "    loss_bilstm_val = df_bilstm['Validation Loss']\n",
    "\n",
    "    epochs_gru = df_gru['Epoch']\n",
    "    loss_gru_train = df_gru['Training Loss']\n",
    "    loss_gru_val = df_gru['Validation Loss']\n",
    "\n",
    "    # Membuat plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # # # Plot Loss Pelatihan\n",
    "    # plt.plot(epochs_lstm[ep1:ep2], loss_lstm_train[ep1:ep2], label='LSTM', color='blue')\n",
    "    # plt.plot(epochs_bilstm[ep1:ep2], loss_bilstm_train[ep1:ep2], label='BiLSTM', color='orange')\n",
    "    # plt.plot(epochs_gru[ep1:ep2], loss_gru_train[ep1:ep2], label='GRU', color='green')\n",
    "\n",
    "    # Plot Loss Pengujian\n",
    "    plt.plot(epochs_lstm[ep1:ep2], loss_lstm_val[ep1:ep2], label='SSA-LSTM', color='blue')\n",
    "    plt.plot(epochs_bilstm[ep1:ep2], loss_bilstm_val[ep1:ep2], label='SSA-BiLSTM', color='orange')\n",
    "    plt.plot(epochs_gru[ep1:ep2], loss_gru_val[ep1:ep2], label='SSA-GRU', color='green')\n",
    "\n",
    "    # Menambahkan Label dan Legenda\n",
    "    plt.title(f'Kurva pelatihan SSA-LSTM, SSA-BiLSTM, and SSA-GRU (time step {time_step})')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Error (MSE data normalisasi)')\n",
    "\n",
    "    if ep1 == 0:\n",
    "        plt.legend()\n",
    "\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/Best/loss_latih_gabungan_{time_step}.png\")\n",
    "    plt.savefig(f\"Output/Grafik/Evaluasi/Best/loss_latih_gabungan_{time_step}.pdf\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
